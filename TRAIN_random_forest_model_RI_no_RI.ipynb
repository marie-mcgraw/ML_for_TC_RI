{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274ace21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NUM_THREADS = \"1\"\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = NUM_THREADS\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = NUM_THREADS\n",
    "os.environ[\"MKL_NUM_THREADS\"] = NUM_THREADS\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = NUM_THREADS\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = NUM_THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a91ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from utils.SHIPS_preprocess import calc_d24_VMAX, fore_hr_averaging, load_processed_SHIPS\n",
    "from utils import SHIPS_ML_model_funcs\n",
    "from utils.SHIPS_ML_model_funcs import get_train_test_split, SHIPS_train_test_shuffle_CLASS, SHIPS_train_test_split\n",
    "from utils import SHIPS_plotting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,confusion_matrix,accuracy_score,precision_score,recall_score,classification_report\n",
    "from sklearn.metrics import precision_recall_curve, auc, f1_score, fbeta_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24287a5",
   "metadata": {},
   "source": [
    "##### Ignore Annoying Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b814f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "#\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action=\"ignore\",category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7b438",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "##### SHIPS Dataset Choice\n",
    "* `max_fore`: maximum forecast hours [usually 24 or 48]\n",
    "* `mask_TYPE`: how are we handling cases close to land? [SIMPLE_MASK or no_MASK]\n",
    "* `interp_str`: Did we interpolate over missing data or not? [INTERP: yes, no_INTERP: no]\n",
    "* `yr_start`:  First year of training data [2010 or 2005, generally]\n",
    "* `yr_end_LOAD`:  Last year of full data (to find file)[2021]\n",
    "* `yr_end_TRAIN`: Last year to use in training [2018 is default]\n",
    "* `use_basin`:  Default is to use all basins, but if we just want to use one basin, we can specify that here [ATLANTIC, EAST_PACIFIC, WEST_PACIFIC, and SOUTHERN_HEM are the choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8bd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fore = 24 # maximum forecast hours\n",
    "mask_TYPE = 'SIMPLE_MASK' # how are we handling the land mask?\n",
    "interp_str = 'INTERP' # did we interpolate?\n",
    "yr_start = 2005\n",
    "yr_end_LOAD = 2021\n",
    "yr_end_TRAIN = 2018\n",
    "use_basin = 'ALL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576116f",
   "metadata": {},
   "source": [
    "#### SHIPS analysis choices\n",
    "* `hrs_max`: maximum forecast hours (usually 24; should be same or less than `max_fore`)\n",
    "* `RI_thresh`: change in wind speed over `hrs_max` needed for RI; default threshold is `30` kt increase in wind speed in `24` hours\n",
    "* `is_RI_only`: flag for future instances of a multi-class classification problem (should always be set to `True` for now)\n",
    "* `n_classes`: related to `is_RI_only`; how many classes are we classifying into (should be `2` for now)\n",
    "* `is_standard`: flag to indicate whether or not we want to do feature scaling with `StandardScaler` (default is `True`)\n",
    "* `DO_AVG`: flag to indicate whether or not we are averaging over our forecast period or treating each 6-hrly forecast as a separate predictor (default is `True`)\n",
    "* `drop_features`: list of features to drop before model training (usually needed for preprocessing but we don't want to train the model on them).  Commonly dropped features include:\n",
    "    * `TYPE`: storm type; should be 1 everywhere (tropical cyclones only)\n",
    "    * `VMAX`: maximum surface winds; we define our classes based entirely on `VMAX` so we don't want it in our features\n",
    "    * `DELV`: we only use `DELV -12` (change in wind speeds from -12 h to 0 h) and not the change in wind speeds relative to 0 for all hours\n",
    "    * `VMPI`: we calculated `POT` (basically `VMPI` - `VMAX_0`) so we don't need to also include `VMPI`\n",
    "    * `is_TRAIN`: just a flag we use to separate training data from validation in our bootstrapped experiments; not an actual feature to train on \n",
    "* `to_IND`: list of quantities we want to index on for our multi-index (note that these quantities will NOT be considered features)\n",
    "    * `BASIN`: ocean basin\n",
    "    * `CASE`: case number\n",
    "    * `NAME`: name of tropical cyclone\n",
    "    * `DATE_full`: date of case (YYYY-MM-DD-HH:MM:SS).  Time stamp is for `time 0`\n",
    "    * `TIME`: forecast time.  should range from `0` to `max_fore_hrs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c32cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs_max = 24\n",
    "# Features to drop before ML model\n",
    "drop_features = {'TYPE','VMAX','DELV','VMPI','is_TRAIN'}\n",
    "to_IND = ['BASIN','CASE','NAME','DATE_full','TIME']\n",
    "RI_thresh = 30\n",
    "is_RI_only = True\n",
    "n_classes = 2\n",
    "is_standard = True\n",
    "if is_standard == True:\n",
    "    stand_str = 'STANDARDIZED'\n",
    "else:\n",
    "    stand_str = 'noSTANDARDIZED'\n",
    "DO_AVG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a6500",
   "metadata": {},
   "source": [
    "#### ML Model Hyperparameters.  This will change based on the type of model\n",
    "* <code>score</code>:  For RF models, this measures quality of a split (called `criterion` in `sklearn`).  Options are `gini` (measures Gini impurity), `log_loss`, and `entropy` (both measure Shannon information gain). \n",
    "* <code>max_features</code>: Maximum number of features per tree.  For classification problems should be approximately $\\sqrt{N_{features}}$.  For us, $\\sqrt{N_{features}} \\approx 4.2$, so we use `[4,5]` as options for `max_features`\n",
    "* `n_estimators`: number of decision trees. We try `[250, 500]`. \n",
    "* `min_samples_leaf`: min. number of samples needed to create a leaf node. We try `[2,4]`\n",
    "* `max_depth`: maximum depth of each decision tree.  We try `[5,6,8]`. Note that generally RF performance improves as `max_depth` increases, but we run the risk of overfitting + model training takes longer, so this is our compromise.  \n",
    "* <code>k_folds</code>: number of folds used in our cross-validation approach.  We will use a <code>Stratified K-Means cross-validation</code> since we have imbalanced classes. Default is `10`\n",
    "* `n_repeats`: number of times we repeat k-folds cross-validation process. Default is `3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267f8d5",
   "metadata": {},
   "source": [
    "##### Class weighting\n",
    "This part is tricky but important.  Since we are really interested in rapid intensification, which is by definition, rare, we will inherently be creating imbalanced classes for our data.  We can address this in many ways.  Broadly, we can either use <b>class weights</b> (which apply different weights to each data point based on which class it is in), or we can use under/over sampling.  <b>Undersampling</b> means we will sample the minority class at a rate that is commeasurate with the majority class--we lose information, but are less likely to overfit to our minority class.  <b>Oversampling</b> means we will draw additional samples from our minority class to match the size of our majority class.  \n",
    "\n",
    "We'll try a few different ways of applying class weights, and we'll try undersampling.  Since our minority classes can be quite small, we will avoid oversampling (for now, at least)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23de004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = ['gini']\n",
    "k_folds = 10\n",
    "n_repeats = 3\n",
    "\n",
    "# Weights\n",
    "use_custom_wts = False\n",
    "# We want to predict intensity class for each case\n",
    "to_predict = 'I_class'\n",
    "# Model hyperparameters\n",
    "max_features = [4,5]\n",
    "max_depth = [5,6,8]\n",
    "min_samples_leaf = [2,4]\n",
    "n_estimators = [250,500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dbedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_format = 'png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee361d2",
   "metadata": {},
   "source": [
    "##### Load our pre-processed SHIPS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b601793",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIPS_predictors,BASIN = load_processed_SHIPS(yr_start,yr_end_LOAD,mask_TYPE,max_fore,interp_str,use_basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d833080",
   "metadata": {},
   "source": [
    "##### Calculate class weights, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d924e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_custom_wts:\n",
    "    class_wts = calculate_class_weights(SHIPS_predictors,n_classes,RI_thresh,0)\n",
    "    weights_use = class_wts.xs(use_basin)\n",
    "    wts_sel = weights_use['WEIGHT'].to_dict()\n",
    "    wts_str = 'custom_wts'\n",
    "else:\n",
    "    wts_sel = 0\n",
    "    wts_str = 'default_wts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ca94d",
   "metadata": {},
   "source": [
    "##### Bootstrapped model training\n",
    "First, initialize some dataframes for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "738e4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_ALL = pd.DataFrame()\n",
    "roc_vals_ALL = pd.DataFrame()\n",
    "p_vs_r_ALL = pd.DataFrame()\n",
    "fi_pred_ALL = pd.DataFrame()\n",
    "fi_pred_train_ALL = pd.DataFrame()\n",
    "cm_ALL = pd.DataFrame()\n",
    "report_ALL = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dcb7f",
   "metadata": {},
   "source": [
    "Next, outline our bootstrapping experiments\n",
    "###### Bootstrapping parameters:\n",
    "\n",
    "* `N_samples`: number of experiments\n",
    "* `ncats`: number of categories for classification (default is 2)\n",
    "* `scoring`: scoring function for ML model (we typically used `f1_weighted` as it's better for imbalanced classes)\n",
    "* `n_valid`: number of years to use for validation\n",
    "\n",
    "###### Overview\n",
    "1. Of full training period (2005-2018), we randomly select `n_valid` years to use for validation.  We use a modified leave-one-year-out approach (where instead we leave `n_valid` years out. This step is handled by the `get_train_test_split` function.  Thus we divide our SHIPS predictors as well as our target variable into training and validation samples based on year. \n",
    "2. We set up a hyperparameter sweep using `sklearn`'s `gridsearchCV` (contained in `create_gridsearch_RF` function).  For random forest, we explore 4 hyperparameters, identified earlier in this notebook.\n",
    "3. After identifying best hyperparameters, we train (`model.fit()`).  We train once on cases from all ocean basins.\n",
    "4. Once training is complete, we try to predict class of our validation years.  We predict each ocean basin separately, as well as predict all ocean basins combined. We use `get_scores_best_params_RF` to get the hyperparameters for our best model, `get_confusion_matrix_RF` to get the confusion matrix and contingency table stats for our model, `get_feature_importances_RF` to get the feature importances, and `get_roc_AUC` to get the receiver operator curve (ROC) and area under the curve (AUC). \n",
    "5. We save all of the output and repeat the process, selecting new validation years and fully re-training every time until we have done `N_samples` experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef3c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sample  0\n",
      "averaging hours together\n",
      "fitting model\n",
      "calculating scores\n",
      "running  ATLANTIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/mmcgraw/ML_for_TC_RI/utils/SHIPS_ML_model_funcs.py:397: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f1 = (2*p*r)/(p+r)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sample  1\n",
      "averaging hours together\n",
      "fitting model\n",
      "calculating scores\n",
      "running  ATLANTIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/mmcgraw/ML_for_TC_RI/utils/SHIPS_ML_model_funcs.py:397: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f1 = (2*p*r)/(p+r)\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "N_samples = 15\n",
    "ncats = 2\n",
    "scoring = 'f1_weighted'\n",
    "cut = 'equal'\n",
    "FULL_yrs = np.arange(yr_start,yr_end_TRAIN,1)\n",
    "n_valid = 3 # number of years to leave out for validation\n",
    "\n",
    "# Loop through bootstrapping examples\n",
    "for i in np.arange(0,N_samples):\n",
    "    print('running sample ',i)\n",
    "    # Split data into training/validation\n",
    "    test_years = np.random.choice(FULL_yrs,n_valid,replace=False) # years we will use for validation\n",
    "    X_train, X_test, y_train, y_test, feature_names, diff_train, diff_test = SHIPS_ML_model_funcs.get_train_test_split(test_years,SHIPS_predictors,to_predict,\n",
    "                                                                    is_RI_only,to_IND,drop_features,DO_AVG,RI_thresh,hrs_max)\n",
    "    # Set up hyperparameter sweep\n",
    "    LR_model = SHIPS_ML_model_funcs.create_gridsearch_RF(is_standard,score,max_depth,n_estimators,max_features,min_samples_leaf,\n",
    "                                k_folds,n_repeats,use_custom_wts,wts_sel,scoring)\n",
    "    print('fitting model')\n",
    "    # Fit model using training data. We train on cases from all 4 basins\n",
    "    LR_model.fit(X_train,y_train['I_class'])\n",
    "    # \n",
    "    BASIN_all = ['ATLANTIC', 'EAST_PACIFIC', 'WEST_PACIFIC', 'SOUTHERN_HEM','ALL']\n",
    "    print('calculating scores')\n",
    "    # Perform validation.  We'll analyze each basin separately as well as combined scores for all basins together\n",
    "    for basin in BASIN_all:\n",
    "        # basin = 'ATLANTIC'\n",
    "        print('running ',basin)\n",
    "        # Get best hyperparams\n",
    "        report, y_true, y_pred = SHIPS_ML_model_funcs.get_scores_best_params_RF(LR_model,X_test,y_test,basin)\n",
    "        report['Years Out'] = str(test_years)\n",
    "        report['Model'] = 'Random Forest'\n",
    "        report['Fold'] = i\n",
    "        label_names = ['not RI','RI']\n",
    "        # Get confusion matrix\n",
    "        cm_stats = SHIPS_ML_model_funcs.get_confusion_matrix_RF(LR_model,y_true,y_pred,basin,label_names,ncats)\n",
    "        cm_stats['Years Out'] = str(test_years)\n",
    "        cm_stats['Model'] = 'Random Forest'\n",
    "        cm_stats['Fold'] = i\n",
    "        # Get feature importances\n",
    "        fi_pred = SHIPS_ML_model_funcs.get_feature_importances_RF(LR_model,X_train,y_train,basin,scoring)\n",
    "        fi_pred['Years Out'] = str(test_years)\n",
    "        fi_pred['Model'] = 'Random Forest'\n",
    "        fi_pred['Fold'] = i\n",
    "        # Feature importances for training data\n",
    "        fi_pred_train = SHIPS_ML_model_funcs.get_feature_importances_RF(LR_model,X_train,y_train,basin,scoring)\n",
    "        fi_pred_train['Years Out'] = str(test_years)\n",
    "        fi_pred_train['Fold'] = i\n",
    "        fi_pred_train['Model'] = 'Random Forest'\n",
    "        # Get ROC curve and AUC score\n",
    "        ypred_prob, p_vs_r, roc_vals = SHIPS_ML_model_funcs.get_roc_auc(X_test,basin,LR_model,y_test,1,'RI',scoring,'equal')\n",
    "        # Save info like experiment #, years used for validation, etc\n",
    "        p_vs_r['Fold'] = i\n",
    "        p_vs_r['Years Out'] = str(test_years)\n",
    "        p_vs_r['Model'] = 'Random Forest'\n",
    "        roc_vals['Fold'] = i\n",
    "        roc_vals['Years Out'] = str(test_years)\n",
    "        roc_vals['Model'] = 'Random Forest'\n",
    "        # Get actual predictions for target variable Y\n",
    "        if basin != 'ALL':\n",
    "            y_pred_all = y_test.xs(basin).copy()\n",
    "        else:\n",
    "            y_pred_all = y_test.copy()\n",
    "        # Save predicted values of Y\n",
    "        y_pred_all['Y pred'] = y_pred\n",
    "        y_pred_all['Predicted Basin'] = basin\n",
    "        y_pred_all['Model'] = 'Random Forest'\n",
    "        # Get probabilities for 0 (not-RI) and 1 (RI)\n",
    "        y_pred_all['Y pred probab (class: 0)'] = ypred_prob[:,0]\n",
    "        y_pred_all['Y pred probab (class: 1)'] = ypred_prob[:,1]\n",
    "        # Store everything before going on to next experiment\n",
    "        predicted_y_ALL = predicted_y_ALL.append(y_pred_all)\n",
    "        roc_vals_ALL = roc_vals_ALL.append(roc_vals)\n",
    "        p_vs_r_ALL = p_vs_r_ALL.append(p_vs_r)\n",
    "        fi_pred_ALL = fi_pred_ALL.append(fi_pred)\n",
    "        fi_pred_train_ALL = fi_pred_train_ALL.append(fi_pred_train)\n",
    "        cm_ALL = cm_ALL.append(cm_stats)\n",
    "        report_ALL = report_ALL.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4aa3a",
   "metadata": {},
   "source": [
    "##### Create output directory and set up file extensions for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90fd43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For naming purposes\n",
    "predicted_y_ALL['BASIN'] = predicted_y_ALL['Predicted Basin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5555c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '~/SHIPS/SHIPS_clean/Model_Results/'\n",
    "model_type = 'RF'\n",
    "save_dir = save_dir+model_type+'/'\n",
    "save_extension = 'TRAIN_{score}_SHIPS_SIMPLE_RI_vs_no_RI_{yr_start}-{yr_end}_{mask_TYPE}_{stand_str}_RI_thresh_{RI_thresh}'\\\n",
    "'weights_{wts_str}_{N}_samples_{scoring}.csv'.format(score=score[0],yr_start=yr_start,yr_end=yr_end_TRAIN,mask_TYPE=mask_TYPE,\n",
    "                           stand_str=stand_str,RI_thresh=RI_thresh,wts_str=wts_str,N=N_samples,scoring=scoring)\n",
    "\n",
    "save_ext_figs = 'TRAIN_{score}_SHIPS_SIMPLE_RI_vs_no_RI_{yr_start}-{yr_end}_{mask_TYPE}_{stand_str}_RI_thresh_{RI_thresh}'\\\n",
    "'weights_{wts_str}_{N}_samples_{scoring}.{fig_format}'.format(score=score[0],yr_start=yr_start,yr_end=yr_end_TRAIN,mask_TYPE=mask_TYPE,\n",
    "                           stand_str=stand_str,RI_thresh=RI_thresh,wts_str=wts_str,N=N_samples,scoring=scoring,\n",
    "                                                             fig_format=fig_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55484fbc",
   "metadata": {},
   "source": [
    "##### Create subdirectories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac79900",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# figs directory\n",
    "if not os.path.exists(save_dir+'/figs/'):\n",
    "    os.makedirs(save_dir+'/figs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c8458",
   "metadata": {},
   "source": [
    "##### Actually save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_ALL.to_csv(save_dir+'PREDICTED_Y_vals'+save_extension)\n",
    "roc_vals_ALL.to_csv(save_dir+'ROC_AUC_vals'+save_extension)\n",
    "p_vs_r_ALL.to_csv(save_dir+'Prec_vs_recall'+save_extension)\n",
    "fi_pred_ALL.to_csv(save_dir+'Feat_Imp'+save_extension)\n",
    "fi_pred_train_ALL.to_csv(save_dir+'Feat_Imp_TRAIN'+save_extension)\n",
    "cm_ALL.to_csv(save_dir+'Conf_Matrix'+save_extension)\n",
    "report_ALL.to_csv(save_dir+'Class_Report'+save_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1342e05",
   "metadata": {},
   "source": [
    "#### Make some basic plots for this LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51286e4e",
   "metadata": {},
   "source": [
    "###### Precision vs Recall plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f025d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_vs_r_ALL_plt = p_vs_r_ALL.reset_index()#.iloc[::2]\n",
    "#basin_sel = 'ALL'\n",
    "for basin_sel in BASIN_all:\n",
    "    foo = p_vs_r_ALL_plt.set_index(['BASIN']).loc[basin_sel].drop(columns={'index'})\n",
    "    foo2 = foo.copy()\n",
    "    foo2['Thresholds Round'] = foo2['Thresholds'].round(2)\n",
    "    means_plt = foo2.groupby(['Thresholds Round']).mean().reset_index()\n",
    "    fig1,ax1 = plt.subplots(1,1,figsize=(10,6))\n",
    "    sns.lineplot(data=foo2.reset_index(),x='Thresholds',y='Recall',hue='Fold',ax=ax1,alpha=0.25,legend=None)\n",
    "    sns.lineplot(data=foo2.reset_index(),x='Thresholds',y='Precision',hue='Fold',ax=ax1,alpha=0.25,legend=None)\n",
    "    sns.lineplot(data=foo2.reset_index(),x='Thresholds',y='F1',hue='Fold',ax=ax1,alpha=0.25,legend=None)\n",
    "    thresh_min = foo2.reset_index()['Cutoff Threshold'].min()\n",
    "    thresh_max = foo2.reset_index()['Cutoff Threshold'].max()\n",
    "    ax1.axvspan(thresh_min,thresh_max,alpha=0.35,color='xkcd:gray',label='Cutoff Threshold')\n",
    "    sns.lineplot(data=means_plt,x='Thresholds Round',y='Recall',ax=ax1,linewidth=6,color='xkcd:crimson',label='Recall')\n",
    "    sns.lineplot(data=means_plt,x='Thresholds Round',y='Precision',ax=ax1,linewidth=6,color='xkcd:sky blue',label='Precision')\n",
    "    sns.lineplot(data=means_plt,x='Thresholds Round',y='F1',ax=ax1,linewidth=6,color='xkcd:goldenrod',label='F1')\n",
    "    ax1.set_xlabel('Thresholds',fontsize=19)\n",
    "    ax1.set_ylabel('Score',fontsize=19)\n",
    "    ax1.legend(fontsize=13)\n",
    "    ax1.grid()\n",
    "    ax1.set_title('Precision vs Recall, Identifying RI, {basin_sel} Basins, RF model'.format(basin_sel=basin_sel),fontsize=21)\n",
    "    fig1.savefig(save_dir+'figs/P_vs_R_{basin_sel}'.format(basin_sel=basin_sel)+save_ext_figs,format=fig_format,\n",
    "                 dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a57aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3,ax3 = plt.subplots(1,1,figsize=(10,6))\n",
    "sns.boxplot(data=roc_vals_ALL,x='BASIN',y='AUC ROC Score',ax=ax3)\n",
    "ax3.set_ylim([0,1])\n",
    "ax3.set_xticklabels(roc_vals_ALL['BASIN'].unique(),fontsize=16,rotation=60)\n",
    "ax3.set_ylabel('AUC Score',fontsize=18)\n",
    "ax3.set_title('AUC Scores, {solver}'.format(solver='RF'),fontsize=21)\n",
    "fig3.savefig(save_dir+'figs/AUC_scores_all_basins_RF'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dba94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_plot = report_ALL.reset_index().rename(columns={'index':'Scores','0.0':'not RI','1.0':'RI'})\n",
    "report_plt_all = report_plot.set_index(['Scores','BASIN','Fold'])\n",
    "score_sel_ALL = ['recall','precision','f1-score','support']\n",
    "for score_sel in score_sel_ALL:\n",
    "    report_plt_mean = report_plt_all.xs((score_sel)).reset_index()\n",
    "    fig4,ax4 = plt.subplots(1,1,figsize=(10,6))\n",
    "    sns.boxplot(data=report_plt_mean,x='BASIN',y='RI',ax=ax4)\n",
    "    if score_sel == 'support':\n",
    "        ax4.set_ylim([0,400])\n",
    "    else:\n",
    "        ax4.set_ylim([0,1])\n",
    "    ax4.set_ylabel('Classifying RI',fontsize=16)\n",
    "    ax4.set_xlabel(None)\n",
    "    ax4.set_xticklabels(report_plt_mean['BASIN'].unique(),fontsize=15,rotation=40)\n",
    "    ax4.grid()\n",
    "    ax4.set_title(' {score_sel}, Classifying RI Cases, RF'.format(score_sel=score_sel),fontsize=20)\n",
    "    fig4.savefig('Model_Results/RF/{score_sel}_all_samples_RI_cases_RF'+save_ext_figs,\n",
    "                format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1961c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_plt2 = report_plt_all.loc[['precision','recall','f1-score']].mean(level=(0,1)).reset_index()\n",
    "fig5,(ax5a,ax5b) = plt.subplots(2,1,figsize=(10,8))\n",
    "sns.scatterplot(data=report_plt2,x='BASIN',y='not RI',hue='Scores',palette='twilight',s=130,ax=ax5a,alpha=0.7,legend=False)\n",
    "sns.lineplot(data=report_plt2,x='BASIN',y='not RI',hue='Scores',palette='twilight',linewidth=2,ax=ax5a,alpha=0.7)\n",
    "\n",
    "sns.scatterplot(data=report_plt2,x='BASIN',y='RI',hue='Scores',palette='magma',s=130,ax=ax5b,alpha=0.7,legend=False)\n",
    "sns.lineplot(data=report_plt2,x='BASIN',y='RI',hue='Scores',palette='magma',linewidth=2,ax=ax5b,alpha=0.7)\n",
    "\n",
    "ax5a.set_ylim([0,1])\n",
    "ax5b.set_ylim([0,1])\n",
    "ax5a.set_ylabel('not RI',fontsize=18)\n",
    "ax5b.set_ylabel('RI',fontsize=18)\n",
    "ax5a.set_xticklabels(report_plt2['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax5a.set_xlabel(None)\n",
    "ax5b.set_xticklabels(report_plt2['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax5b.set_xlabel(None)\n",
    "ax5a.grid()\n",
    "ax5b.grid()\n",
    "ax5a.legend(fontsize=13)\n",
    "ax5b.legend(fontsize=13)\n",
    "fig5.suptitle('Precision, Recall, and F1 Scores, Averaged Over Bootstrapped Samples, RF',fontsize=20)\n",
    "fig5.tight_layout()\n",
    "fig5.savefig(save_dir+'figs/Scores_averaged_RI_non_RI'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67375c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for basin_sel in BASIN_all:\n",
    "    fig2,ax2 = plt.subplots(1,1,figsize=(10,7))\n",
    "    roc_vals_plt = roc_vals_ALL.set_index(['BASIN']).xs(basin_sel).reset_index()\n",
    "    roc_min = roc_vals_plt['AUC ROC Score'].min()\n",
    "    roc_max = roc_vals_plt['AUC ROC Score'].max()\n",
    "\n",
    "    sns.lineplot(data=roc_vals_plt,x='False Positive Rate',y='True Positive Rate',hue='Fold',ax=ax2,legend=False,\n",
    "                alpha=0.3)\n",
    "    ax2.plot([0,1],[0,1],color='k',linewidth=2)\n",
    "    ax2.axhspan(roc_min,roc_max,color='xkcd:gray',alpha=0.25,label='AUC Score')\n",
    "    ax2.set_xlabel('False Positive Rate',fontsize=18)\n",
    "    ax2.set_ylabel('True Positive Rate',fontsize=18)\n",
    "    roc_vals_mean = roc_vals_plt.groupby(roc_vals_plt['False Positive Rate'].round(2))[['True Positive Rate',\n",
    "                                    'AUC Thresholds']].mean().reset_index()\n",
    "    roc_vals_mean.plot(x='False Positive Rate',y='True Positive Rate',ax=ax2,color='xkcd:tangerine',linewidth=5,\n",
    "                      label='ROC curve')\n",
    "    ax2.legend(fontsize=13)\n",
    "    ax2.grid()\n",
    "    ax2.set_title('Identifying RI versus non-RI, {basin_sel} Basins, {solver} model'.format(basin_sel=basin_sel,\n",
    "                                                                                           solver='RF'),fontsize=21)\n",
    "    f2_save = save_dir+'figs/ROC_curve_{basin_sel}'.format(basin_sel=basin_sel)\n",
    "    fig2.savefig(f2_save+save_ext_figs,format=fig_format,\n",
    "                 dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(data=cm_ALL,x='Category',y='Misses')\n",
    "cm_ALL['BIAS'] = (cm_ALL['Hits']+cm_ALL['False Alarms'])/(cm_ALL['Hits'] + cm_ALL['Misses'])\n",
    "\n",
    "fig6,((ax6a,ax6b),(ax6c,ax6d)) = plt.subplots(2,2,figsize=(14,10))\n",
    "sns.boxplot(data=cm_ALL,x='BASIN',y='Misses',hue='Category Names',palette='twilight',ax=ax6a)\n",
    "ax6a.set_ylabel('Misses',fontsize=15)\n",
    "ax6a.legend(fontsize=12)\n",
    "ax6a.set_xticklabels(cm_ALL['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax6a.set_title('Misses',fontsize=19)\n",
    "ax6a.set_xlabel(None)\n",
    "#\n",
    "sns.boxplot(data=cm_ALL,x='BASIN',y='Hits',hue='Category Names',palette='twilight',ax=ax6b)\n",
    "ax6b.set_ylabel('Hits',fontsize=15)\n",
    "ax6b.legend(fontsize=12)\n",
    "ax6b.set_xticklabels(cm_ALL['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax6b.set_title('Hits',fontsize=19)\n",
    "ax6b.set_xlabel(None)\n",
    "#\n",
    "sns.boxplot(data=cm_ALL,x='BASIN',y='POD',hue='Category Names',palette='twilight',ax=ax6c)\n",
    "ax6c.set_ylabel('POD',fontsize=15)\n",
    "ax6c.legend(fontsize=12)\n",
    "ax6c.set_xticklabels(cm_ALL['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax6c.set_title('Probability of Detection',fontsize=19)\n",
    "ax6c.set_xlabel(None)\n",
    "#\n",
    "#\n",
    "sns.boxplot(data=cm_ALL,x='BASIN',y='Threat',hue='Category Names',palette='twilight',ax=ax6d)\n",
    "ax6d.set_ylabel('Threat Score',fontsize=15)\n",
    "ax6d.legend(fontsize=12)\n",
    "ax6d.set_xticklabels(cm_ALL['BASIN'].unique(),fontsize=14,rotation=30)\n",
    "ax6d.set_title('Threat Score',fontsize=19)\n",
    "ax6d.set_xlabel(None)\n",
    "#\n",
    "\n",
    "fig6.suptitle('{solver} Model'.format(solver='RF'),fontsize=21)\n",
    "fig6.tight_layout()\n",
    "fig6.savefig(save_dir+'figs/CM_results_RI_not_RI_RF'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc409e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7,ax7 = plt.subplots(1,1,figsize=(12,8))\n",
    "fi_plt = fi_pred_ALL.reset_index().sort_values(['BASIN','mean importance'],ascending=False)\n",
    "fi_plt['mean mean'] = fi_plt.groupby(['BASIN','index'])['mean importance'].transform('mean')\n",
    "fi_plt['max mean'] = fi_plt.groupby(['BASIN','index'])['mean importance'].transform('max')\n",
    "fi_plt_plt = fi_plt.sort_values(['BASIN','mean mean','max mean'],ascending=[True,False,False])\n",
    "sns.barplot(data=fi_plt_plt,x='index',y='mean importance',hue='BASIN',\n",
    "            palette='twilight',ax=ax7)\n",
    "ax7.set_xticklabels(fi_plt_plt['index'].unique(),fontsize=14,rotation=50)\n",
    "ax7.set_ylabel('Mean Importance',fontsize=17)\n",
    "ax7.set_xlabel(None)\n",
    "ax7.grid()\n",
    "ax7.tick_params(axis='y',labelsize=14)\n",
    "ax7.legend(fontsize=13)\n",
    "ax7.set_title('Feature Importances, TRAINING DATA, not-RI vs RI, {solver}'.format(solver='RF'),fontsize=21)\n",
    "fig7.tight_layout()\n",
    "fig7.savefig(save_dir+'figs/Feat_Imp_RI_not_RI'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b8e1b",
   "metadata": {},
   "source": [
    "##### Each basin separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for basin_sel in BASIN_all:\n",
    "#basin_sel = 'ATLANTIC'\n",
    "    i_plt = fi_pred_ALL.reset_index().set_index(['BASIN']).xs(basin_sel).sort_values(['mean importance'],ascending=False)\n",
    "    i_plt['mean mean'] = i_plt.groupby(['index'])['mean importance'].transform('mean')\n",
    "    i_plt['max mean'] = i_plt.groupby(['index'])['mean importance'].transform('max')\n",
    "    i_plt_plt = i_plt.sort_values(['mean mean','max mean'],ascending=[False,False])\n",
    "    fig7,ax7 = plt.subplots(1,1,figsize=(15,10))\n",
    "\n",
    "    sns.barplot(data=i_plt_plt,x='index',y='mean importance',\n",
    "                palette='twilight',ax=ax7)\n",
    "    ax7.set_xticklabels(i_plt_plt['index'].unique(),fontsize=14,rotation=50)\n",
    "    ax7.set_ylabel('Mean Importance',fontsize=17)\n",
    "    ax7.tick_params(axis='y',labelsize=14)\n",
    "    ax7.set_xlabel(None)\n",
    "    ax7.grid()\n",
    "    #ax7.legend(fontsize=13)\n",
    "    ax7.set_title('Feature Importances, not-RI vs RI, {solver}, {basin}'.format(solver='RF',basin=basin_sel),fontsize=21)\n",
    "    fig7.tight_layout()\n",
    "    fig7.savefig(save_dir+'figs/Feat_Imp_RI_not_RI_{basin_sel}'.format(basin_sel=basin_sel)+save_ext_figs,\n",
    "                format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100658ab",
   "metadata": {},
   "source": [
    "###### FI Based on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88358425",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7,ax7 = plt.subplots(1,1,figsize=(15,10))\n",
    "fi_plt = fi_pred_train_ALL.reset_index().sort_values(['BASIN','mean importance'],ascending=False)\n",
    "fi_plt['mean mean'] = fi_plt.groupby(['BASIN','index'])['mean importance'].transform('mean')\n",
    "fi_plt['max mean'] = fi_plt.groupby(['BASIN','index'])['mean importance'].transform('max')\n",
    "fi_plt_plt = fi_plt.sort_values(['BASIN','mean mean','max mean'],ascending=[True,False,False])\n",
    "sns.barplot(data=fi_plt_plt,x='index',y='mean importance',hue='BASIN',\n",
    "            palette='twilight',ax=ax7)\n",
    "ax7.set_xticklabels(fi_plt_plt['index'].unique(),fontsize=14,rotation=50)\n",
    "ax7.set_ylabel('Mean Importance',fontsize=17)\n",
    "ax7.set_xlabel(None)\n",
    "ax7.grid()\n",
    "ax7.tick_params(axis='y',labelsize=14)\n",
    "ax7.legend(fontsize=13)\n",
    "ax7.set_title('Feature Importances, TRAINING DATA, not-RI vs RI, {solver}'.format(solver='RF'),fontsize=21)\n",
    "fig7.tight_layout()\n",
    "fig7.savefig(save_dir+'figs/Feat_Imp_TRAIN_RI_not_RI'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811fc65",
   "metadata": {},
   "source": [
    "##### And each basin separately again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for basin_sel in BASIN_all:\n",
    "#basin_sel = 'ATLANTIC'\n",
    "    i_plt = fi_pred_train_ALL.reset_index().set_index(['BASIN']).xs(basin_sel).sort_values(['mean importance'],ascending=False)\n",
    "    i_plt['mean mean'] = i_plt.groupby(['index'])['mean importance'].transform('mean')\n",
    "    i_plt['max mean'] = i_plt.groupby(['index'])['mean importance'].transform('max')\n",
    "    i_plt_plt = i_plt.sort_values(['mean mean','max mean'],ascending=[False,False])\n",
    "    fig7,ax7 = plt.subplots(1,1,figsize=(15,10))\n",
    "\n",
    "    sns.barplot(data=i_plt_plt,x='index',y='mean importance',\n",
    "                palette='twilight',ax=ax7)\n",
    "    ax7.set_xticklabels(i_plt_plt['index'].unique(),fontsize=14,rotation=50)\n",
    "    ax7.set_ylabel('Mean Importance',fontsize=17)\n",
    "    ax7.tick_params(axis='y',labelsize=14)\n",
    "    ax7.set_xlabel(None)\n",
    "    ax7.grid()\n",
    "    #ax7.legend(fontsize=13)\n",
    "    ax7.set_title('Feature Importances, TRAINING DATA, not-RI vs RI, {solver}, {basin}'.format(solver='RF',basin=basin_sel),fontsize=21)\n",
    "    fig7.tight_layout()\n",
    "    fig7.savefig(save_dir+'figs/Feat_Imp_RI_not_RI_TRAINING_{basin_sel}'.format(basin_sel=basin_sel)+save_ext_figs,\n",
    "                format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e20278",
   "metadata": {},
   "source": [
    "##### Performance diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12,ax12 = plt.subplots(1,1,figsize=(12,8))\n",
    "make_performance_diagram_background(ax12)\n",
    "\n",
    "#ax12.errorbar(cm_ALL_PD_sel.reset_index()['SR'],cm_ALL_PD_sel.reset_index()['POD'],yerr=cm_ALL_yerr,xerr=cm_ALL_xerr,\n",
    " #           linestyle='none',linewidth=2,color='k')\n",
    "#sns.scatterplot(data=cm_ALL_PD_sel.reset_index(),x='SR',y='POD',hue='BASIN',ax=ax12,palette=sns.set_palette(pal_sel),\n",
    "  #              s=180,zorder=10)\n",
    "ax12.set_title('Classifying RI Cases, {solver}'.format(solver='RF'),fontsize=22)\n",
    "add_model_results(ax12,cm_ALL)\n",
    "fig12.savefig(save_dir+'figs/Performance_Diagram'+save_ext_figs,\n",
    "           format='png',dpi=250,bbox_inches='tight')\n",
    "plt.close('all')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cc1de",
   "metadata": {},
   "source": [
    "#### Performance Diagram curves: PoD vs Success Ratio\n",
    "\n",
    "1. PoD vs Success Ratio curves.  Each fold shown separately, and then averaged across all folds.  Each basin will be separate.\n",
    "2. AUPD (area under performance diagram) scores. Calculated for each fold and shown as a swarm or box plot, all basins on one plot\n",
    "3. Max CSI. Calculated for each fold / basin and shown as swarm or box plot, all basins on one plot. \n",
    "4. CSI vs Bias.  Calculated for each fold / basin and shown as scatterplot, all basins on one plot. Also show mean across all folds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_curves = calculate_PD_curves(p_vs_r_ALL)\n",
    "fig15,ax15 = plt.subplots(1,1,figsize=(10,6))\n",
    "max_CSI_ind = p_vs_r_ALL.groupby(['BASIN','Fold'])[['CSI','Bias']].agg({'CSI':'max'}).reset_index()\n",
    "plot_basic_score_basin(ax15,max_CSI_ind,'CSI',False)\n",
    "ax15.set_title('Maximum CSI Scores for RI Cases, {solver}'.format(solver='RF'),fontsize=20)\n",
    "ax15.set_ylabel('Maximum CSI Score',fontsize=16)\n",
    "ax15.set_ylim([0,0.6])\n",
    "fig15.savefig('Model_Results/RF/Max_CSI_RI_vs_basin'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.swarmplot(data=foo2.reset_index(),x='BASIN',y='CSI')\n",
    "fig30,ax30 = plt.subplots(1,1,figsize=(10,6))\n",
    "plot_CSI_vs_bias(p_vs_r_ALL,ax30)\n",
    "ax30.set_title('Bias at Maximum CSI for RI cases, {solver}'.format(solver='RF'),fontsize=21)\n",
    "fig30.savefig(save_dir+'figs/CSI_vs_bias_RI'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121de230",
   "metadata": {},
   "source": [
    "##### Area under PD Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03959c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "aupd_scores = calc_AUPD(p_vs_r_ALL)\n",
    "aupd_plt = aupd_scores.reset_index().rename(columns={0:'AUPD'})\n",
    "fig14,ax14 = plt.subplots(1,1,figsize=(10,6))\n",
    "plot_basic_score_basin(ax14,aupd_plt,'AUPD',True)\n",
    "ax14.set_ylabel('Area Under PD Curve',fontsize=16)\n",
    "ax14.set_title('Area Under Performance Diagram, RI Cases, {solver}'.format(solver='RF'),fontsize=21)\n",
    "ax14.set_ylim([0,0.55])\n",
    "fig14.savefig(save_dir+'figs/AUPD_calculation_RI_cases'+save_ext_figs,\n",
    "            format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2844951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "for basin_sel in BASIN_all:\n",
    "    fig23,ax23 = plt.subplots(1,1,figsize=(12,8))\n",
    "    make_performance_diagram_background(ax23)\n",
    "    plot_PD_curves(p_vs_r_ALL,ax23,basin_sel)\n",
    "    ax23.set_title('RI Cases, {basin_sel}, {solver}'.format(basin_sel=basin_sel,solver='RF'),fontsize=22)\n",
    "    f23_save = (save_dir+'figs/Performance_Diagram_CURVES_{basin_sel}'.format(basin_sel=basin_sel)\n",
    "    fig23.savefig(f23_save+save_ext_figs,\n",
    "                format=fig_format,dpi=300,bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18c207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHIPS",
   "language": "python",
   "name": "ships"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
